{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7b9536e",
   "metadata": {},
   "source": [
    "### Split up the dataset into 3 chunks\n",
    "\n",
    "We want to compare training on the full dataset with OSFT versus training on 3 chunks in a sequence.\n",
    "\n",
    "This notebook handles the data conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fb7b2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--. 1 oleg oleg 44522306 Jul 31 02:34 /mnt/nvme1n1/experiments/os-cl-scenario-1-experiment-0/bmo/train_p07.jsonl\n"
     ]
    }
   ],
   "source": [
    "!ls -al /mnt/nvme1n1/experiments/os-cl-scenario-1-experiment-0/bmo/train_p07.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01ee331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory for saving\n",
    "!mkdir -p /mnt/nvme1n1/experiments/os-cl-scenario-1-experiment-0/bmo/3-chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a10ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import math\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "# local_dataset = \"/mnt/nvme1n1/experiments/os-cl-scenario-1-experiment-0/bmo/train_p07.jsonl\"\n",
    "# ds = datasets.load_dataset(\"json\", data_files=local_dataset, split=\"train\")\n",
    "\n",
    "def split_dataset(ds: datasets.Dataset, num_chunks: int) -> list[datasets.Dataset]:\n",
    "    split_size = round(len(ds) / num_chunks)\n",
    "    ordered_chunks = []\n",
    "    for i in range(0, len(ds), split_size):\n",
    "        selection = ds.select(range(i, min(i + split_size, len(ds))))\n",
    "        ordered_chunks += [selection]\n",
    "    \n",
    "    assert sum(len(c) for c in ordered_chunks) == len(ds)\n",
    "    return ordered_chunks\n",
    "\n",
    "# load the dataset and split it per-chunk\n",
    "chunk_range = range(2, 12)\n",
    "ds_range = range(2, 235)\n",
    "for N_chunks in chunk_range:\n",
    "    for ds_size in ds_range:\n",
    "        # for debugging\n",
    "        nums = [{\"num\": k} for k in range(17)]\n",
    "        fake_ds = datasets.Dataset.from_list(nums)\n",
    "        ds = fake_ds\n",
    "\n",
    "        ordered_chunks = split_dataset(fake_ds, N_chunks)\n",
    "\n",
    "        assert sum(len(c) for c in ordered_chunks) == len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5652a84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\n",
      "drwxr-xr-x. 2 oleg oleg 4096 Jul 31 03:57 .\n",
      "drwxr-xr-x. 3 oleg oleg 4096 Jul 31 03:57 ..\n"
     ]
    }
   ],
   "source": [
    "!ls -al /mnt/nvme1n1/experiments/os-cl-scenario-1-experiment-0/bmo/3-chunks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ae24d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing ds to /mnt/nvme1n1/experiments/os-cl-scenario-1-experiment-0/bmo/3-chunks/chunk_0.jsonl...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a0ea8f42c4499ab2e86ea4c3938072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing ds to /mnt/nvme1n1/experiments/os-cl-scenario-1-experiment-0/bmo/3-chunks/chunk_1.jsonl...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0517b6146b34eaaa02beda1b5a8cd24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing ds to /mnt/nvme1n1/experiments/os-cl-scenario-1-experiment-0/bmo/3-chunks/chunk_2.jsonl...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9907d3115ca14d2f81be87095bc32aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading chunk from /mnt/nvme1n1/experiments/os-cl-scenario-1-experiment-0/bmo/3-chunks/chunk_0.jsonl...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ef72cf751643579efd856c41781b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading chunk from /mnt/nvme1n1/experiments/os-cl-scenario-1-experiment-0/bmo/3-chunks/chunk_1.jsonl...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510b0976a2d646a390208e920c9cb5da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading chunk from /mnt/nvme1n1/experiments/os-cl-scenario-1-experiment-0/bmo/3-chunks/chunk_2.jsonl...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9757e47c0b2493590cef95e97491554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully validated that chunks can be loaded back in same order\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# okay now we copy the same formula\n",
    "N_chunks = 3\n",
    "local_dataset = \"/mnt/nvme1n1/experiments/os-cl-scenario-1-experiment-0/bmo/train_p07.jsonl\"\n",
    "ds = datasets.load_dataset(\"json\", data_files=local_dataset, split=\"train\")\n",
    "ds_chunks = split_dataset(ds, N_chunks)\n",
    "assert sum(len(chunk) for chunk in ds_chunks) == len(ds)\n",
    "\n",
    "# save the dataset\n",
    "output_dir = \"/mnt/nvme1n1/experiments/os-cl-scenario-1-experiment-0/bmo/3-chunks\"\n",
    "\n",
    "# write out the datasets\n",
    "for i, ds_chunk in enumerate(ds_chunks):\n",
    "    filename = f\"chunk_{i}.jsonl\"\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    print(f\"writing ds to {output_path}...\")\n",
    "    ds_chunk.to_json(output_path)\n",
    "\n",
    "# now load the chunks back in order and validate\n",
    "loaded_chunks = []\n",
    "for i in range(N_chunks):\n",
    "    chunk_path = os.path.join(output_dir, f\"chunk_{i}.jsonl\")\n",
    "    print(f\"loading chunk from {chunk_path}...\")\n",
    "    chunk_ds = datasets.load_dataset(\"json\", data_files=chunk_path, split=\"train\")\n",
    "    loaded_chunks.append(chunk_ds)\n",
    "    \n",
    "# concatenate all chunks\n",
    "reconstructed_ds = datasets.concatenate_datasets(loaded_chunks)\n",
    "    \n",
    "# validate length matches\n",
    "assert len(reconstructed_ds) == len(ds), f\"Length mismatch: {len(reconstructed_ds)} != {len(ds)}\"\n",
    "    \n",
    "# validate contents match exactly\n",
    "for i in range(len(ds)):\n",
    "    assert ds[i] == reconstructed_ds[i], f\"Mismatch at index {i}\"\n",
    "    \n",
    "print(\"Successfully validated that chunks can be loaded back in same order\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
